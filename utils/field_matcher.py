import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
import os
import json
from typing import List, Dict, Optional, Tuple, Set,Union,Any
from collections import defaultdict
import difflib
import unicodedata
from sentence_transformers import SentenceTransformer

import re
from config.config import FORM_HISTORY_PATH

# Äáº£m báº£o cÃ¡c tÃ i nguyÃªn NLTK Ä‘Æ°á»£c táº£i xuá»‘ng
def ensure_nltk_resources():
    resources = {
        'punkt_tab': 'tokenizers/punkt_tab', 
        'stopwords': 'corpora/stopwords', 
        'omw-1.4': 'corpora/omw-1.4'
    }
    for resource, path in resources.items():
        try:
            nltk.data.find(path)
            print(f"Resource {resource} already exists.")
        except LookupError:
            try:
                print(f"Downloading {resource} resource...")
                nltk.download(resource, quiet=True)
                print(f"Downloaded {resource} successfully!")
            except Exception as e:
                print(f"Error downloading {resource}: {e}")
                print(f"Please manually download {resource} using: nltk.download('{resource}')")

# Táº£i cÃ¡c resource cáº§n thiáº¿t
ensure_nltk_resources()

class EnhancedFieldMatcher:
    def __init__(self, form_history_path: str):
        self.form_history_path = form_history_path
        self.user_preferences = defaultdict(dict)
        self._load_user_preferences()  # (user_id, field, value) -> frequency
        self.synonym_map = self._build_synonym_map()
        self.stop_words = self._initialize_stopwords()
        self.field_name_cache = {}
        self.field_value_mapping = defaultdict(list)
        self.vectorizer = None
        self.word2vec_model = None
        self.field_vectors = None
        self.field_names = []
        self.field_embeddings = {}
        self.matched_fields = {}
        
        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u lá»‹ch sá»­
        
        self._build_field_value_mapping()
        self._build_models()
    def _load_user_preferences(self):
        """Táº£i preferences cá»§a tá»«ng user dá»±a trÃªn lá»‹ch sá»­"""
        for form in self.form_history_path:
            if isinstance(form, dict) and 'user_id' in form:
                user_id = form['user_id']
                if 'form_data' in form:
                    for field_name, value in form['form_data'].items():
                        if value and str(value).strip():
                            # LÆ°u cáº£ táº§n suáº¥t sá»­ dá»¥ng vÃ  giÃ¡ trá»‹ thÆ°á»ng dÃ¹ng
                            if field_name not in self.user_preferences[user_id]:
                                self.user_preferences[user_id][field_name] = {
                                    'count': 0,
                                    'values': defaultdict(int)
                                }
                            self.user_preferences[user_id][field_name]['count'] += 1
                            self.user_preferences[user_id][field_name]['values'][str(value).strip()] += 1

    def _get_user_field_boost(self, user_id: int, field_name: str) -> float:
        """TÃ­nh Ä‘iá»ƒm boost dá»±a trÃªn táº§n suáº¥t sá»­ dá»¥ng cá»§a user"""
        if user_id in self.user_preferences and field_name in self.user_preferences[user_id]:
            usage_count = self.user_preferences[user_id][field_name]['count']
            # Boost tá»‘i Ä‘a 0.2 cho cÃ¡c trÆ°á»ng thÆ°á»ng dÃ¹ng
            return min(usage_count * 0.01, 0.2)
        return 0.0
    def _build_synonym_map(self) -> Dict[str, List[str]]:
        raw_map = {
            'há» tÃªn': [
                'hovaten', 'ho va ten', 'ho vÃ  tÃªn', 'hoten', 'há»_tÃªn', 'tÃªn', 
                'há» vÃ  tÃªn', 'fullname', 'full name', 'name', 'your name', 'tÃªn Ä‘áº§y Ä‘á»§'
            ],
            'Ä‘á»‹a chá»‰': [
                'diachi', 'Ä‘á»‹a_chá»‰', 'Ä‘á»‹a chá»‰', 'address', 'place', 'location', 
                'home address', 'residence', 'chá»— á»Ÿ', 'nÆ¡i á»Ÿ', 'current address', 'address line'
            ],
            'Ä‘iá»‡n thoáº¡i': [
                'sdt', 'so_dien_thoai', 'so dien thoai', 'Ä‘t', 'phone', 'tel', 
                'telephone', 'mobile', 'mobile phone', 'phone number', 'sá»‘ Ä‘iá»‡n thoáº¡i', 'contact number'
            ],
            'email': [
                'e-mail', 'mail', 'email address', 'email', 'Ä‘á»‹a chá»‰ email', 'thÆ° Ä‘iá»‡n tá»­'
            ],
            'ngÃ y sinh': [
                "ngÃ y sinh", "dob", "birth date", "birthdate", "date of birth", 
                "d.o.b", "ngay_sinh", "birth"
            ],
            'giá»›i tÃ­nh': [
                'gender', 'sex', 'gioi_tinh', 'gioi tinh', 'giá»›i tÃ­nh', 'male/female'
            ],
            'mÃ£ sá»‘ thuáº¿': [
                'tax code', 'mÃ£ thuáº¿', 'mst', 'tax id'
            ],
            'quá»‘c tá»‹ch': [
                'nationality', 'country'
            ],
            'thÃ nh phá»‘': [
                'city', 'tá»‰nh thÃ nh', 'tá»‰nh/thÃ nh phá»‘'
            ],
            'quáº­n huyá»‡n': [
                'district', 'huyá»‡n', 'quáº­n'
            ],
            'phÆ°á»ng xÃ£': [
                'ward', 'xÃ£', 'phÆ°á»ng'
            ],
            'chuyÃªn nghÃ nh': [
                'há»c nghÃ nh', 'nghÃ nh'
            ]

        }
        return raw_map


    def _calculate_sbert_similarity(self, text1: str, text2: str) -> float:
        vec1 = self.sbert_model.encode([text1])[0]
        vec2 = self.sbert_model.encode([text2])[0]
        return cosine_similarity([vec1], [vec2])[0][0]
    def _initialize_stopwords(self) -> Set[str]:
        """Khá»Ÿi táº¡o stopwords cho cáº£ tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t"""
        english_stopwords = set(stopwords.words('english'))
        vietnamese_stopwords = {
            'cá»§a', 'vÃ ', 'cÃ¡c', 'cÃ³', 'Ä‘Æ°á»£c', 'trong', 'lÃ ', 'cho', 'nhá»¯ng', 'vá»›i',
            'khÃ´ng', 'nÃ y', 'Ä‘áº¿n', 'khi', 'vá»', 'nhÆ°', 'tá»«', 'má»™t', 'ngÆ°á»i', 'nÄƒm',
            'bá»‹', 'Ä‘Ã£', 'sáº½', 'cÅ©ng', 'vÃ o', 'ra', 'náº¿u', 'Ä‘á»ƒ', 'táº¡i', 'theo',
            'sau', 'trÃªn', 'hoáº·c', 'tÃ´i', 'báº¡n', 'anh', 'chá»‹', 'há»', 'cá»§a', 'mÃ¬nh'
        }
        return english_stopwords.union(vietnamese_stopwords)
    
    def _load_form_history(self) -> List[Dict]:
        """Táº£i lá»‹ch sá»­ biá»ƒu máº«u tá»« file JSON"""
        if os.path.exists(self.form_history_path):
            try:
                with open(self.form_history_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"Error loading form history: {e}")
                return []
        return []
    
    def _build_field_value_mapping(self):
        """XÃ¢y dá»±ng Ã¡nh xáº¡ giá»¯a tÃªn trÆ°á»ng vÃ  cÃ¡c giÃ¡ trá»‹ Ä‘Ã£ Ä‘iá»n"""
        for form in self.form_history_path:
            if isinstance(form, dict) and 'form_data' in form:
                form_data = form['form_data']
                # Bá» qua cÃ¡c trÆ°á»ng Ä‘áº·c biá»‡t
                special_fields = {'form_id', 'document_name'}
                for field_name, value in form_data.items():
                    if field_name not in special_fields and value and (val_str := str(value).strip()):
                        self.field_value_mapping[field_name].append(val_str)
    
    def _preprocess_text(self, text: str) -> str:
        """Tiá»n xá»­ lÃ½ vÄƒn báº£n nÃ¢ng cao cho tiáº¿ng Viá»‡t"""
        if not text:
            return ""
            
        # Chuáº©n hÃ³a Unicode vÃ  chuyá»ƒn Ä‘á»•i vá» chá»¯ thÆ°á»ng
        text = unicodedata.normalize('NFC', text.lower())
        
        # Loáº¡i bá» dáº¥u cÃ¢u vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i dáº¥u tiáº¿ng Viá»‡t
        text = re.sub(r'[^\w\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]', ' ', text)
        
        # Thay tháº¿ tá»« Ä‘á»“ng nghÄ©a phá»• biáº¿n
        for target, synonyms in self.synonym_map.items():
            for synonym in synonyms:
                text = re.sub(r'\b' + re.escape(synonym) + r'\b', target, text)
        
        # Loáº¡i bá» stopwords
        tokens = text.split()
        filtered_tokens = [token for token in tokens if token not in self.stop_words]
        
        return ' '.join(filtered_tokens)
    
    def _normalize_field_name(self, field_name: str) -> str:
        """Chuáº©n hÃ³a tÃªn trÆ°á»ng nÃ¢ng cao"""
        if not field_name:
            return ""
        
        # Chuáº©n hÃ³a Unicode vÃ  chuyá»ƒn Ä‘á»•i vá» chá»¯ thÆ°á»ng
        text = unicodedata.normalize('NFC', field_name.lower())
        
        # Thay tháº¿ tá»« Ä‘á»“ng nghÄ©a - cáº£i tiáº¿n Ä‘á»ƒ Æ°u tiÃªn thay tháº¿ cá»¥m tá»« dÃ i trÆ°á»›c
        synonym_items = sorted(self.synonym_map.items(), key=lambda x: max(len(s) for s in x[1]), reverse=True)
        for target, synonyms in synonym_items:
            for synonym in sorted(synonyms, key=len, reverse=True):
                text = re.sub(r'\b' + re.escape(synonym) + r'\b', target, text)
        
        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
        text = re.sub(r'[^\w\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]', ' ', text)
        
        # Loáº¡i bá» stopwords
        tokens = [token for token in text.split() if token not in self.stop_words]
        
        return ' '.join(tokens).strip()
    
    def _build_models(self):
        """XÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh TF-IDF vÃ  Word2Vec"""
        # Thu tháº­p táº¥t cáº£ cÃ¡c tÃªn trÆ°á»ng Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½
        processed_fields = []
        field_names = []
        
        for form in self.form_history_path:
            if isinstance(form, dict) and 'form_data' in form:
                for field_name in form['form_data'].keys():
                    if field_name not in self.field_name_cache:
                        processed = self._preprocess_text(field_name)
                        self.field_name_cache[field_name] = processed
                        processed_fields.append(processed)
                        field_names.append(field_name)
        
        # XÃ¢y dá»±ng TF-IDF vectorizer
        if processed_fields:
            self.vectorizer = TfidfVectorizer()
            self.field_vectors = self.vectorizer.fit_transform(processed_fields)
            self.field_names = field_names
            
            # XÃ¢y dá»±ng Word2Vec model
            sentences = [field.split() for field in processed_fields if field.split()]
            if sentences:
                self.word2vec_model = Word2Vec(
                    sentences, 
                    vector_size=100, 
                    window=5, 
                    min_count=1, 
                    workers=4,
                    epochs=20
                )
                
                # Táº¡o embeddings cho má»—i trÆ°á»ng
                for field, processed in zip(field_names, processed_fields):
                    tokens = processed.split()
                    if tokens:
                        # TÃ­nh trung bÃ¬nh cÃ¡c vector tá»« (chá»‰ láº¥y tokens tá»“n táº¡i trong vocab)
                        embeddings = []
                        for token in tokens:
                            if token in self.word2vec_model.wv:
                                embeddings.append(self.word2vec_model.wv[token])
                        
                        if embeddings:  # Chá»‰ gÃ¡n náº¿u cÃ³ Ã­t nháº¥t 1 embedding
                            self.field_embeddings[field] = np.mean(embeddings, axis=0)
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tá»•ng há»£p sá»­ dá»¥ng nhiá»u phÆ°Æ¡ng phÃ¡p"""
        # Chuáº©n hÃ³a vÄƒn báº£n
        norm1 = self._normalize_field_name(text1)
        norm2 = self._normalize_field_name(text2)
       
        # 1. SequenceMatcher similarity
        seq_sim = difflib.SequenceMatcher(None, norm1, norm2).ratio()
        
        # 2. TF-IDF cosine similarity
        tfidf_sim = 0.0
        if self.vectorizer and self.field_vectors is not None:
            try:
                query_vec = self.vectorizer.transform([self._preprocess_text(text1)])
                target_vec = self.vectorizer.transform([self._preprocess_text(text2)])
                tfidf_sim = cosine_similarity(query_vec, target_vec)[0][0]
            except Exception as e:
                print(f"TF-IDF similarity error: {e}")
        
        # 3. Word2Vec similarity (náº¿u cÃ³ embeddings)
        w2v_sim = 0.0
        if self.word2vec_model:
            try:
                tokens1 = self._preprocess_text(text1).split()
                tokens2 = self._preprocess_text(text2).split()
                
                if tokens1 and tokens2:
                    # Kiá»ƒm tra xem táº¥t cáº£ tokens cÃ³ trong vocab khÃ´ng
                    if all(token in self.word2vec_model.wv for token in tokens1) and \
                       all(token in self.word2vec_model.wv for token in tokens2):
                        w2v_sim = self.word2vec_model.wv.n_similarity(tokens1, tokens2)
            except Exception as e:
                print(f"Word2Vec similarity error: {e}")
        
        # Káº¿t há»£p cÃ¡c Ä‘iá»ƒm similarity vá»›i trá»ng sá»‘
        sbert_sim = self._calculate_sbert_similarity(text1, text2)
        combined_sim = 0.25 * seq_sim + 0.25 * tfidf_sim + 0.2 * w2v_sim + 0.3 * sbert_sim
        
        return combined_sim
    def _boost_by_frequency(self, field_name: str, base_score: float) -> float:
        frequency = len(self.field_value_mapping.get(field_name, []))
        boost = min(frequency / 10, 1.0)  # Giá»›i háº¡n boost khÃ´ng vÆ°á»£t quÃ¡ 1.0
        return base_score + 0.05 * boost  # TÄƒng nháº¹ Ä‘iá»ƒm
    def match_fields(
        self,
        form_model: Union[str, List[str]],
        threshold: float = 0.65,
        user_id: Optional[str] = None
    ) -> Dict[str, Dict[str, Any]]:
        """
        GhÃ©p cÃ¡c trÆ°á»ng giá»¯a form_model vÃ  form_data sá»­ dá»¥ng káº¿t há»£p nhiá»u phÆ°Æ¡ng phÃ¡p.
        Tráº£ vá» dict gá»“m tÃªn trÆ°á»ng Ä‘Ã£ khá»›p, tÃªn trÆ°á»ng dá»¯ liá»‡u khá»›p, vÃ  giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng.
        """
        with open("form_history.json", "r", encoding="utf-8") as f:
            history_data = json.load(f)

        if not history_data:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u trong form_history.json")
            return {}

        # Chuyá»ƒn form_model thÃ nh list náº¿u lÃ  chuá»—i
        if isinstance(form_model, str):
            form_model = [form_model]

        print(f"\nðŸ§© Danh sÃ¡ch trÆ°á»ng cáº§n ghÃ©p: {form_model}")
        if user_id:
            print(f"ðŸ”‘ Chá»‰ xÃ©t cÃ¡c báº£n ghi cá»§a user_id: {user_id}")

        user_records = [record for record in history_data if record.get("user_id") == user_id]
        user_records = list(reversed(user_records))  # Duyá»‡t tá»« má»›i nháº¥t

        if not user_records:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y báº£n ghi nÃ o thuá»™c user_id nÃ y.")
            return {}

        for idx, record in enumerate(user_records):
            form_data = record.get("form_data", {})

            print(f"\nðŸ“„ Äang kiá»ƒm tra báº£n ghi thá»© {idx + 1}/{len(user_records)}: {len(form_data)} trÆ°á»ng")

            matched_fields = {}
            used_model_fields = set()
            used_data_fields = set()
            potential_matches = []

            for model_field in form_model:
                for data_field in form_data.keys():
                    similarity = self._calculate_similarity(model_field, data_field)
                    similarity += self._boost_by_frequency(data_field, similarity)
                    similarity += self._exact_token_match_boost(model_field, data_field)

                    if similarity >= threshold:
                        potential_matches.append((similarity, model_field, data_field))

            # Sáº¯p xáº¿p theo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giáº£m dáº§n
            potential_matches.sort(reverse=True, key=lambda x: x[0])

            for similarity, model_field, data_field in potential_matches:
                if model_field not in used_model_fields and data_field not in used_data_fields:
                    matched_fields[model_field] = {
                        "matched_field": data_field,
                        "value": form_data[data_field]
                    }
                    used_model_fields.add(model_field)
                    used_data_fields.add(data_field)

            if matched_fields:
                for model, match in matched_fields.items():
                    print(f"   - {model} <-- {match['matched_field']} : {match['value']}")
                self.matched_fields = matched_fields
                return matched_fields

            else:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ trong báº£n ghi nÃ y. Tiáº¿p tá»¥c tÃ¬m trong báº£n ghi khÃ¡c...")

        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p trong báº¥t ká»³ báº£n ghi nÃ o cá»§a user_id nÃ y.")
        self.matched_fields = {}
        return self.matched_fields


    def _exact_token_match_boost(self, model_field: str, data_field: str) -> float:
        model_tokens = set(self._preprocess_text(model_field).split())
        data_tokens = set(self._preprocess_text(data_field).split())
        overlap = model_tokens & data_tokens
        return 0.1 * len(overlap)
    def find_most_similar_field(self, query: str, top_n: int = 3) -> List[Tuple[str, float]]:
        """
        TÃ¬m cÃ¡c trÆ°á»ng tÆ°Æ¡ng tá»± nháº¥t vá»›i query sá»­ dá»¥ng káº¿t há»£p TF-IDF vÃ  Word2Vec
        """
        if not query or not self.field_names or not self.vectorizer:
            return []
            
        try:
            # Tiá»n xá»­ lÃ½ query
            processed_query = self._preprocess_text(query)
            
            # 1. TÃ¬m kiáº¿m báº±ng TF-IDF
            tfidf_results = []
            if self.vectorizer and self.field_vectors is not None:
                query_vec = self.vectorizer.transform([processed_query])
                similarities = cosine_similarity(query_vec, self.field_vectors).flatten()
                tfidf_results = [(self.field_names[i], float(similarities[i])) 
                                for i in np.argsort(similarities)[-top_n:][::-1]
                                if similarities[i] > 0]
            
            # 2. TÃ¬m kiáº¿m báº±ng Word2Vec (náº¿u cÃ³ model)
            w2v_results = []
            if self.word2vec_model and self.field_embeddings:
                query_tokens = processed_query.split()
                if query_tokens:
                    # TÃ­nh embedding cho query (chá»‰ láº¥y tokens tá»“n táº¡i trong vocab)
                    valid_tokens = [token for token in query_tokens if token in self.word2vec_model.wv]
                    if valid_tokens:
                        query_embedding = np.mean([self.word2vec_model.wv[token] for token in valid_tokens], axis=0)
                        
                        # TÃ­nh similarity vá»›i cÃ¡c trÆ°á»ng Ä‘Ã£ biáº¿t
                        similarities = {}
                        for field, embedding in self.field_embeddings.items():
                            if embedding is not None:  # Chá»‰ tÃ­nh similarity náº¿u cÃ³ embedding
                                sim = cosine_similarity(
                                    query_embedding.reshape(1, -1), 
                                    embedding.reshape(1, -1)
                                )[0][0]
                                similarities[field] = sim
                        
                        # Láº¥y top_n káº¿t quáº£
                        w2v_results = sorted(
                            [(field, sim) for field, sim in similarities.items()], 
                            key=lambda x: x[1], 
                            reverse=True
                        )[:top_n]
            
            # Káº¿t há»£p vÃ  xáº¿p háº¡ng káº¿t quáº£
            combined_results = defaultdict(float)
            
            # ThÃªm Ä‘iá»ƒm tá»« TF-IDF
            for field, score in tfidf_results:
                combined_results[field] += score * 0.5
                
            # ThÃªm Ä‘iá»ƒm tá»« Word2Vec
            for field, score in w2v_results:
                combined_results[field] += score * 0.5
            
            # Sáº¯p xáº¿p vÃ  tráº£ vá» káº¿t quáº£
            final_results = sorted(
                [(field, score) for field, score in combined_results.items()],
                key=lambda x: x[1],
                reverse=True
            )[:top_n]
            
            return final_results
            
        except Exception as e:
            print(f"Error finding similar fields: {e}")
            return []
    
    def get_suggested_values(self, field_name: str, limit: int = 3, 
                           user_id: Optional[int] = None) -> List[str]:
        """
        Cáº£i tiáº¿n: Æ¯u tiÃªn giÃ¡ trá»‹ mÃ  user Ä‘Ã£ tá»«ng nháº­p
        """
        # 1. Láº¥y giÃ¡ trá»‹ chung
        all_values = []
        direct_values = self.field_value_mapping.get(field_name, [])
        all_values.extend([(v, 1.0) for v in direct_values])  # Weight = 1.0 cho giÃ¡ trá»‹ chung
        
        # 2. ThÃªm giÃ¡ trá»‹ tá»« user náº¿u cÃ³
        if user_id is not None and user_id in self.user_preferences:
            if field_name in self.user_preferences[user_id]:
                user_values = self.user_preferences[user_id][field_name]['values']
                for val, count in user_values.items():
                    # TÄƒng weight cho giÃ¡ trá»‹ cá»§a user (count * 2.0 Ä‘á»ƒ Æ°u tiÃªn hÆ¡n)
                    all_values.append((val, count * 2.0))
        
        # 3. ThÃªm giÃ¡ trá»‹ tá»« cÃ¡c trÆ°á»ng tÆ°Æ¡ng tá»±
        similar_fields = self.find_most_similar_field(field_name, top_n=3)
        for similar_field, _ in similar_fields:
            if similar_field != field_name:
                similar_values = self.field_value_mapping.get(similar_field, [])
                all_values.extend([(v, 0.7) for v in similar_values])  # Weight tháº¥p hÆ¡n cho giÃ¡ trá»‹ tÆ°Æ¡ng tá»±
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng há»£p
        value_scores = defaultdict(float)
        for val, weight in all_values:
            value_scores[val] += weight
        
        # Sáº¯p xáº¿p theo Ä‘iá»ƒm vÃ  Ä‘á»™ dÃ i
        sorted_values = sorted(
            value_scores.items(),
            key=lambda x: (-x[1], len(x[0])),
            reverse=False
        )
        
        return [val[0] for val in sorted_values[:limit]]
    
    def update_form_history(self, new_form_data: Dict, user_id: Optional[int] = None) -> None:
        """
        Cáº£i tiáº¿n: Cáº­p nháº­t cáº£ user preferences khi thÃªm dá»¯ liá»‡u má»›i
        """
        try:
            # ThÃªm thÃ´ng tin user náº¿u cÃ³
            if user_id is not None:
                new_form_data['user_id'] = user_id
            
            # ThÃªm vÃ o lá»‹ch sá»­
            self.form_history_path.append({'form_data': new_form_data, 'user_id': user_id})
            
            # Cáº­p nháº­t field_value_mapping
            for field_name, value in new_form_data.items():
                if value and str(value).strip():
                    val_str = str(value).strip()
                    self.field_value_mapping[field_name].append(val_str)
                    
                    # Cáº­p nháº­t user preferences
                    if user_id is not None:
                        if field_name not in self.user_preferences[user_id]:
                            self.user_preferences[user_id][field_name] = {
                                'count': 0,
                                'values': defaultdict(int)
                            }
                        self.user_preferences[user_id][field_name]['count'] += 1
                        self.user_preferences[user_id][field_name]['values'][val_str] += 1
            
            # XÃ¢y dá»±ng láº¡i models
            self._build_models()
            
            # LÆ°u vÃ o file
            with open(self.form_history_path, 'w', encoding='utf-8') as f:
                json.dump(self.form_history_path, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error updating form history: {e}")